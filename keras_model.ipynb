{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKbxcLerhbhk",
        "outputId": "2b9149a0-5dae-4419-de5b-945ca3deb006"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/sj/dacon/code_similarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuT-wtm9hgrT",
        "outputId": "5b1a034d-b029-40b4-eaae-c77afcdd6965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/sj/dacon/code_similarity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_addons\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8r9ATPcjxX6",
        "outputId": "fa8fe702-80c7-4f2d-adf8-769e689d567f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typeguard>=2.7\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow_addons) (23.0)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.19.0 typeguard-2.13.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 KB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.0 tokenizers-0.13.2 transformers-4.26.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.3.5)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4.0,>=2.0\n",
            "  Downloading charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, charset-normalizer, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 charset-normalizer-3.1.0 datasets-2.10.1 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import sys\n",
        "import sklearn\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from transformers import AutoTokenizer, TFT5EncoderModel\n",
        "from datasets import load_dataset, load_metric, Dataset\n",
        "from itertools import combinations"
      ],
      "metadata": {
        "id": "5aihTSm3h2WN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"Salesforce/codet5-base\"\n",
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 4"
      ],
      "metadata": {
        "id": "aPFLGZC2kKN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 주석, 빈 줄, import문 제거\n",
        "def remove_annotation(code):\n",
        "    line_list = []\n",
        "\n",
        "    single_quote = False\n",
        "    double_quote = False\n",
        "    for line in code.split(\"\\n\"):\n",
        "        if single_quote:\n",
        "            if \"'''\" not in line:\n",
        "                continue\n",
        "\n",
        "            single_quote = False\n",
        "            line = line.split(\"'''\")[1]\n",
        "        elif double_quote:\n",
        "            if '\"\"\"' not in line:\n",
        "                continue\n",
        "\n",
        "            double_quote = False\n",
        "            line = line.split('\"\"\"')[1]\n",
        "\n",
        "        if line.startswith(\"!\"):\n",
        "            continue\n",
        "        elif line.startswith(\"import \"):\n",
        "            continue\n",
        "        elif line.startswith(\"from \"):\n",
        "            continue\n",
        "\n",
        "        annotation_idx = sys.maxsize\n",
        "        single_quote_idx = sys.maxsize\n",
        "        double_quote_idx = sys.maxsize\n",
        "\n",
        "        try:\n",
        "            annotation_idx = line.index(\"#\")\n",
        "        except:\n",
        "            pass\n",
        "        try:\n",
        "            single_quote_idx = line.index(\"'''\")\n",
        "        except:\n",
        "            pass\n",
        "        try:\n",
        "            double_quote_idx = line.index('\"\"\"')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        if annotation_idx < single_quote_idx and annotation_idx < double_quote_idx:\n",
        "            no_annotation_line = line.split(\"#\")[0]\n",
        "            if no_annotation_line.strip() != \"\":\n",
        "                line_list.append(no_annotation_line)\n",
        "        elif single_quote_idx < annotation_idx and single_quote_idx < double_quote_idx:\n",
        "            single_quote = True\n",
        "            no_single_quote_line = line.split(\"'''\")[0]\n",
        "            if no_single_quote_line.strip() != \"\":\n",
        "                line_list.append(no_single_quote_line)\n",
        "        elif double_quote_idx < annotation_idx and double_quote_idx < single_quote_idx:\n",
        "            double_quote = True\n",
        "            no_double_quote_line = line.split('\"\"\"')[0]\n",
        "            if no_double_quote_line.strip() != \"\":\n",
        "                line_list.append(no_double_quote_line)\n",
        "        else:\n",
        "            if line.strip() != \"\":\n",
        "                line_list.append(line)\n",
        "\n",
        "    return \"\\n\".join(line_list)\n",
        "\n",
        "def preprocess_code(file_loc):\n",
        "    line_list = []\n",
        "    with open(file_loc, \"r\", encoding=\"utf-8\") as file:\n",
        "        code = file.read()\n",
        "        modified_code = remove_annotation(code)\n",
        "\n",
        "    return modified_code\n",
        "\n",
        "def parse_code(problem_list, max_file_count=20, start_idx=0):\n",
        "    code_list = []\n",
        "    problem_idx = 0\n",
        "    for problem_dir in tqdm(problem_list):\n",
        "        python_files = glob.glob(f\"{problem_dir}/*.py\")\n",
        "        file_count = 0\n",
        "        for python_file in python_files[start_idx:]:\n",
        "            code = preprocess_code(python_file)\n",
        "            code_list.append((code, problem_idx))\n",
        "            file_count = file_count + 1\n",
        "            if file_count == max_file_count:\n",
        "                break\n",
        "        problem_idx += 1\n",
        "\n",
        "    return pd.DataFrame(code_list, columns=[\"code\", \"problem_idx\"])\n",
        "\n",
        "def remove_invalid_code(df):\n",
        "    df[\"tokens\"] = df[\"code\"].apply(tokenizer.tokenize)\n",
        "    df[\"len\"] = df[\"tokens\"].apply(len)\n",
        "    df = df[df[\"len\"] > 4].reset_index(drop=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "def train_test_split(df, split_rate=0.1):\n",
        "    problem_list = df[\"problem_idx\"].unique().tolist()\n",
        "    valid_idx = random.sample(problem_list, int(len(problem_list) * split_rate))\n",
        "\n",
        "    train_df = df[~df[\"problem_idx\"].isin(valid_idx)]\n",
        "    valid_df = df[df[\"problem_idx\"].isin(valid_idx)]\n",
        "\n",
        "    return train_df, valid_df\n",
        "\n",
        "def random_combinations(list1, list2, count):\n",
        "    total_group = set()\n",
        "    while len(total_group) < count:\n",
        "        item1 = list1[random.randrange(0, len(list1))]\n",
        "        item2 = list2[random.randrange(0, len(list2))]\n",
        "\n",
        "        total_group.add((item1, item2))\n",
        "\n",
        "    return total_group\n",
        "\n",
        "def generate_pairs(df, sample_rate=0.01):\n",
        "    total_positive_pairs = []\n",
        "    total_negative_pairs = []\n",
        "\n",
        "    total_data = []\n",
        "    problem_list = df[\"problem_idx\"].unique().tolist()\n",
        "    for problem_idx in tqdm(problem_list):\n",
        "        solution_codes = df[df[\"problem_idx\"] == problem_idx][\"code\"]\n",
        "        negative_codes = df[df[\"problem_idx\"] != problem_idx][\"code\"]\n",
        "        positive_pairs = list(combinations(solution_codes.to_list(), 2))\n",
        "        sampled_positive_pairs = random.sample(positive_pairs, len(positive_pairs) // int(1 / sample_rate))\n",
        "\n",
        "        negative_pairs = random_combinations(solution_codes.to_list(), negative_codes.to_list(), len(sampled_positive_pairs))\n",
        "        \n",
        "        true_data = [(x[0], x[1], 1) for x in sampled_positive_pairs]\n",
        "        false_data = [(x[0], x[1], 0) for x in negative_pairs]\n",
        "\n",
        "        total_pairs = true_data + false_data\n",
        "        total_data.extend(total_pairs)\n",
        "\n",
        "    pair_df = pd.DataFrame(total_data, columns=[\"code1\", \"code2\", \"similar\"])\n",
        "    pair_df = sklearn.utils.shuffle(pair_df)\n",
        "\n",
        "    return pair_df\n",
        "\n",
        "def generate_datasets(max_file_count, start_idx, train_sr=0.2, valid_sr=0.1):\n",
        "    random.seed(42)\n",
        "\n",
        "    dataset_dirs = [\"code\"]\n",
        "\n",
        "    total_problem_dirs = []\n",
        "    for dataset_dir in dataset_dirs:\n",
        "        problem_dirs = glob.glob(f\"{dataset_dir}/*\")\n",
        "        total_problem_dirs.extend(problem_dirs)\n",
        "\n",
        "    df = parse_code(total_problem_dirs, max_file_count=max_file_count, start_idx=start_idx)\n",
        "    #df = remove_invalid_code(df)\n",
        "    train_df, valid_df = train_test_split(df, split_rate=0.1)\n",
        "\n",
        "    train_pair_df = generate_pairs(train_df, sample_rate=train_sr)\n",
        "    valid_pair_df = generate_pairs(valid_df, sample_rate=valid_sr)\n",
        "\n",
        "    return train_pair_df, valid_pair_df"
      ],
      "metadata": {
        "id": "Kr-6hWxbkPST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def preprocess_data(examples):\n",
        "    code1_output = tokenizer(examples['code1'], padding=True, max_length=MAX_LEN, truncation=True)\n",
        "    code2_output = tokenizer(examples['code2'], padding=True, max_length=MAX_LEN, truncation=True)\n",
        "\n",
        "    examples[\"code1_input_ids\"] = code1_output[\"input_ids\"]\n",
        "    examples[\"code1_attention_mask\"] = code1_output[\"attention_mask\"]\n",
        "\n",
        "    examples[\"code2_input_ids\"] = code2_output[\"input_ids\"]\n",
        "    examples[\"code2_attention_mask\"] = code2_output[\"attention_mask\"]\n",
        "\n",
        "    return examples\n",
        "\n",
        "train_df, valid_df = generate_datasets(150, 0, train_sr=0.1, valid_sr=0.01)\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "valid_dataset = Dataset.from_pandas(valid_df)\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess_data, remove_columns=[\"code1\", \"code2\"])\n",
        "valid_dataset = valid_dataset.map(preprocess_data, remove_columns=[\"code1\", \"code2\"])\n",
        "\n",
        "def convert_to_tensorflow_dataset(dataset):\n",
        "    dataset.set_format(type='tensorflow', columns=[\"code1_input_ids\", \"code1_attention_mask\", \"code2_input_ids\", \"code2_attention_mask\", \"similar\"])\n",
        "    features = {x: dataset[x].to_tensor(default_value=0, shape=[None, MAX_LEN]) for x in [\"code1_input_ids\", \"code1_attention_mask\", \"code2_input_ids\", \"code2_attention_mask\"]}\n",
        "    labels = tf.keras.utils.to_categorical(dataset[\"similar\"])\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "train_ds = convert_to_tensorflow_dataset(train_dataset)\n",
        "val_ds = convert_to_tensorflow_dataset(valid_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "wDJZaIi2kTAb",
        "outputId": "b131d41c-ab16-49f9-a0e5-742f997e02b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 37%|███▋      | 110/300 [03:50<06:38,  2.10s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c1070c596d5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_sr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_sr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-de69ad0be1bb>\u001b[0m in \u001b[0;36mgenerate_datasets\u001b[0;34m(max_file_count, start_idx, train_sr, valid_sr)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mtotal_problem_dirs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem_dirs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_problem_dirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_file_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_file_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;31m#df = remove_invalid_code(df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-de69ad0be1bb>\u001b[0m in \u001b[0;36mparse_code\u001b[0;34m(problem_list, max_file_count, start_idx)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mfile_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpython_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpython_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mcode_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproblem_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mfile_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_count\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-de69ad0be1bb>\u001b[0m in \u001b[0;36mpreprocess_code\u001b[0;34m(file_loc)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mline_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mmodified_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/codecs.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, errors)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0mbyte\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \"\"\"\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'strict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0mIncrementalDecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;31m# undecoded input that is kept between calls to decode()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = []\n",
        "\n",
        "CHECKPOINT_DIR_PREFIX = os.path.join(\"checkpoint\", MODEL_NAME.replace(\"/\", \"-\"))\n",
        "os.makedirs(CHECKPOINT_DIR_PREFIX, exist_ok=True)\n",
        "\n",
        "latest_checkpoint_path = os.path.join(CHECKPOINT_DIR_PREFIX, \"latest.ckpt\")\n",
        "latest_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=latest_checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    save_freq=\"epoch\"\n",
        ")\n",
        "callbacks.append(latest_checkpoint_callback)\n",
        "\n",
        "current_time = time.strftime(\"%y%m%d-%H%M%S\")\n",
        "checkpoint_path = os.path.join(CHECKPOINT_DIR_PREFIX, current_time, \"cp-{epoch:04d}.ckpt\")\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    verbose=1,\n",
        "    save_weights_only=True,\n",
        "    save_freq=\"epoch\"\n",
        ")\n",
        "callbacks.append(checkpoint_callback)\n",
        "\n",
        "log_path = os.path.join(CHECKPOINT_DIR_PREFIX, f\"{MODEL_NAME.replace('/', '-')}-{current_time}.csv\")\n",
        "logger_callback = tf.keras.callbacks.CSVLogger(\n",
        "    log_path, separator=',', append=True\n",
        ")\n",
        "callbacks.append(logger_callback)\n",
        "\n",
        "earlystop_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    verbose=1,\n",
        "    patience=4,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "callbacks.append(earlystop_callback)\n",
        "\n",
        "auto_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.8,\n",
        "    patience=2,\n",
        "    verbose=1,\n",
        "    min_lr=1e-9\n",
        ")\n",
        "callbacks.append(auto_lr_callback)"
      ],
      "metadata": {
        "id": "DacsyGa0kYdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TFRobertaForSequenceClassification 모델에서 가져옴\n",
        "class ClassificationHead(tf.keras.layers.Layer):\n",
        "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
        "\n",
        "    def __init__(self, output_count, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = tf.keras.layers.Dense(\n",
        "            768,\n",
        "            kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),\n",
        "            activation=\"tanh\",\n",
        "            name=\"dense\",\n",
        "        )\n",
        "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "        self.out_proj = tf.keras.layers.Dense(\n",
        "            output_count, kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02), name=\"out_proj\"\n",
        "        )\n",
        "\n",
        "    def call(self, features, training=False):\n",
        "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
        "        x = self.dropout(x, training=training)\n",
        "        x = self.dense(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "        x = self.out_proj(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "8yv44nl_nR2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code1_input_ids_shape = (256)\n",
        "code1_input_ids_input = tf.keras.Input(\n",
        "    shape=code1_input_ids_shape, dtype=tf.int32, name=\"code1_input_ids\"\n",
        ")\n",
        "\n",
        "code1_attention_mask_shape = (256)\n",
        "code1_attention_mask_input = tf.keras.Input(\n",
        "    shape=code1_attention_mask_shape, dtype=tf.int64, name=\"code1_attention_mask\"\n",
        ")\n",
        "\n",
        "code2_input_ids_shape = (256)\n",
        "code2_input_ids_input = tf.keras.Input(\n",
        "    shape=code2_input_ids_shape, dtype=tf.int32, name=\"code2_input_ids\"\n",
        ")\n",
        "\n",
        "code2_attention_mask_shape = (256)\n",
        "code2_attention_mask_input = tf.keras.Input(\n",
        "    shape=code2_attention_mask_shape, dtype=tf.int64, name=\"code2_attention_mask\"\n",
        ")\n",
        "\n",
        "pretrained_model = TFT5EncoderModel.from_pretrained(MODEL_NAME, from_pt=True)\n",
        "classification_head = ClassificationHead(output_count=2)\n",
        "\n",
        "input_ids_input = tf.keras.layers.Concatenate()([code1_input_ids_input, code2_input_ids_input])\n",
        "attention_mask_input = tf.keras.layers.Concatenate()([code1_attention_mask_input, code2_attention_mask_input])\n",
        "code_output = pretrained_model(input_ids=input_ids_input, attention_mask=attention_mask_input)[0]\n",
        "classification_output = classification_head(code_output)\n",
        "dense_softmax = tf.keras.layers.Activation(\"softmax\")(classification_output)\n",
        "\n",
        "model = tf.keras.Model(\n",
        "    inputs=[code1_input_ids_input, code1_attention_mask_input, code2_input_ids_input, code2_attention_mask_input],\n",
        "    outputs=dense_softmax,\n",
        ")\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=[\"accuracy\", tfa.metrics.F1Score(2)])"
      ],
      "metadata": {
        "id": "vtamG5LZnaVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "6jjp63Xgng9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.0842\n",
        "history = model.fit(train_ds,\n",
        "          validation_data=val_ds,\n",
        "          callbacks=callbacks,\n",
        "          epochs=100)"
      ],
      "metadata": {
        "id": "5VkHxE9qnjEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights(latest_checkpoint_path)"
      ],
      "metadata": {
        "id": "5-3upMpGnmQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(latest_checkpoint_path)"
      ],
      "metadata": {
        "id": "BaOC_llynmX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_preprocess_data(examples):\n",
        "    code1 = remove_annotation(examples['code1'])\n",
        "    code2 = remove_annotation(examples['code2'])\n",
        "\n",
        "    code1_output = tokenizer(code1, padding=True, max_length=MAX_LEN, truncation=True)\n",
        "    code2_output = tokenizer(code2, padding=True, max_length=MAX_LEN, truncation=True)\n",
        "\n",
        "    examples[\"code1_input_ids\"] = code1_output[\"input_ids\"]\n",
        "    examples[\"code1_attention_mask\"] = code1_output[\"attention_mask\"]\n",
        "\n",
        "    examples[\"code2_input_ids\"] = code2_output[\"input_ids\"]\n",
        "    examples[\"code2_attention_mask\"] = code2_output[\"attention_mask\"]\n",
        "\n",
        "    return examples\n",
        "\n",
        "def test_convert_to_tensorflow_dataset(dataset):\n",
        "    dataset.set_format(type='tensorflow', columns=[\"code1_input_ids\", \"code1_attention_mask\", \"code2_input_ids\", \"code2_attention_mask\"])\n",
        "    features = {x: dataset[x].to_tensor(default_value=0, shape=[None, MAX_LEN]) for x in [\"code1_input_ids\", \"code1_attention_mask\", \"code2_input_ids\", \"code2_attention_mask\"]}\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((features))\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "test_dataset = load_dataset(\"csv\", data_files=\"test.csv\")[\"train\"]\n",
        "test_dataset = test_dataset.map(test_preprocess_data, remove_columns=[\"code1\", \"code2\"])\n",
        "test_ds = test_convert_to_tensorflow_dataset(test_dataset)\n",
        "\n",
        "preds = model.predict(test_ds)\n",
        "\n",
        "submission = pd.read_csv('./sample_submission.csv')\n",
        "submission['similar'] = np.argmax(preds, axis=-1)\n",
        "submission.to_csv('./submission.csv', index=False)"
      ],
      "metadata": {
        "id": "CEE6_Trknmee"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}